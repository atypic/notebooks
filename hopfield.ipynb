{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopfield network ###\n",
    "\n",
    "This is a very short tutorial on Hopfield Networks (HN) by me, the interpretations are my own and so are the mistakes. I tend to ramble on, so you'll have to beer with me...\n",
    "\n",
    "Hopfield networks consist of all-to-all connected nodes with symmetric and non-recursive weights between the nodes. \n",
    "\n",
    "They can be 'trained' or 'set up' to perform associative memory tasks, i.e. if you have somehow stored a pattern in the network, this pattern can be recalled by presenting a noisy version of the same pattern as inputs to the network and let it converge for a while. The formulation (symmetric weights and non-self-connections) generally ensures convergence to some \"energy minima\", or rather a Lyapunov minima.\n",
    "\n",
    "#### Intuition ####\n",
    "How does the network remember? Essentially what is happening is that the information contained in the pattern is encoded in the weights between the nodes. The weights represent various gradients in a landscape. When you present a input, or \"starting point\" in the landscape, the update-step of the algorithm takes a look at the weights and decides how to move in the landscape to get to the \"bottom\" as it were. This is not far removed from what happens in the training scenario of a perceptron.\n",
    "\n",
    "How do we find these gradients, or weights? Lots of different methods, most conceptually simple is Hebbian. This rule says that \"neurons that wire together, fire together\"-- translated to weights this means that when you present an input pattern, say  [-1, -1], if these two neurons have the same state then the weights between then should be increased. The rule is thus simply that: For all neuron-to-neuron-connections, if the neurons have the same state, add 1 to the weight.\n",
    "\n",
    "If you present a noisy version of the pattern to the network after training; the network implicitly \"knows\" that certain neurons are supposed to have the same value OR opposing values. e.g. if you give a [1, -1] then when we go to calculate the updated value of the first node, which is essentially all the weights connected to this node multiplied by the value of the node it's coming from, the high weights will to a certain extent \"overrule\" the state of the \"coming from\" node-- the weights say you're supposed to fire, and fire you shall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "\n",
    "W = np.zeros((N,N))\n",
    "np.fill_diagonal(W, 0)\n",
    "\n",
    "T = np.zeros((N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s[-1. -1.  0.  0.  0.]\n",
      "s[-1. -1.  1. -1. -1.]\n",
      "s[-1. -1.  1. -1. -1.]\n",
      "s[-1. -1.  1. -1. -1.]\n",
      "s[-1. -1.  1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "p = np.array([-1,-1, 1,-1,-1])\n",
    "\n",
    "#hebbian learning rule, aka \"wire together, fire together\"\n",
    "for (i,j), wi in np.ndenumerate(W):\n",
    "    if i == j:\n",
    "        continue\n",
    "    #if neighbors are same; weight between nodes is increased. (also for negative because -1*-1 = 1\n",
    "    #essentially you are coding that \"these two nodes have the same value\"\n",
    "    #with a weight.\n",
    "    W[(i,j)] += p[i] * p[j]  \n",
    "    W[(j,i)] += p[i] * p[j]  \n",
    "    \n",
    "init_state = p\n",
    "p[0] = 1\n",
    "p[1] = 1\n",
    "\n",
    "S = init_state\n",
    "for t in range(5):\n",
    "    #since the weights contain coding for similar patterns;\n",
    "    #if we present a noisy version of the learned pattern,\n",
    "    #there's a high chance that some of the state pattern are similar,\n",
    "    #and this will in turn drive the state space towards the learned pattern.\n",
    "    S = np.sign(np.dot(W, S) - T)    #(1,N) @ (N,N) = 1,N \n",
    "    print(f\"s{S}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1.  1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
